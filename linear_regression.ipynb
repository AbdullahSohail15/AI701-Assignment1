{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0wg1F8fGFnJO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jWbYWatzQf5X"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions\n",
        "\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "dataset = load_iris()\n",
        "sample = [ 57, 122, 118,  53, 117,  81,  70, 142,  84,  16, 103,  82,  66, 31,  83]\n",
        "test_sample = [89, 20, 72, 67, 12]\n",
        "\n",
        "petal_length = np.take(dataset.data[:, 2], sample)\n",
        "petal_width = np.take(dataset.data[:, 3], sample)\n",
        "iris_data = Bunch(data=petal_length, target=petal_width)\n",
        "\n",
        "test_petal_length = np.take(dataset.data[:, 2], test_sample)\n",
        "test_petal_width = np.take(dataset.data[:, 3], test_sample)\n",
        "\n",
        "def plot_iris_dataset(a=None, b=None, model=None, residuals=False, test=False, savefig=False, figname=None):\n",
        "  if test:\n",
        "    plt.scatter(iris_data.data, iris_data.target, alpha=0.2, zorder=0)\n",
        "    plt.scatter(test_petal_length, test_petal_width, c='red', zorder=0)\n",
        "  else:\n",
        "    plt.scatter(iris_data.data, iris_data.target, zorder=0)\n",
        "\n",
        "  x = np.linspace(0, 8, 100).reshape(-1, 1)\n",
        "  if model:\n",
        "    plt.plot(x, model.predict(x), c='orange', zorder=0)\n",
        "  elif a is not None and b is not None:\n",
        "    plt.plot(x, a * x + b, c='orange', zorder=0)\n",
        "\n",
        "\n",
        "  if residuals:\n",
        "    if test:\n",
        "      X = test_petal_length.reshape(-1, 1)\n",
        "      y = test_petal_width\n",
        "    else:\n",
        "      X = iris_data.data.reshape(-1, 1)\n",
        "      y = iris_data.target\n",
        "    if model:\n",
        "      y_pred = model.predict(X)\n",
        "    elif a is not None and b is not None:\n",
        "      y_pred = a * X + b\n",
        "    plt.vlines(X, y, y_pred, colors='red', zorder=1)\n",
        "\n",
        "  plt.xlabel(\"petal length (cm)\")\n",
        "  plt.ylabel(\"petal width (cm)\")\n",
        "  plt.xlim([-0.25, 8.25])\n",
        "  plt.ylim([-0.25, 2.75])\n",
        "\n",
        "  if savefig:\n",
        "    plt.savefig('{}.jpeg'.format(figname if figname else 'figure'), dpi=600)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_iris_dataset_with_line(a, b, residuals=False, test=False, **kwargs):\n",
        "  plot_iris_dataset(a=a, b=b, residuals=residuals, test=test, **kwargs)\n",
        "\n",
        "# These functions are needed for the 3D dataset\n",
        "def predict_house_price(x1, x2, a):\n",
        "    y = a[0]*x1 + a[1]*x2 + a[2]\n",
        "    return y\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_3d_data_samples(data, a=None, plane=False):\n",
        "  x, y, z = data\n",
        "\n",
        "  fig = go.Figure()\n",
        "\n",
        "  fig.add_trace(go.Scatter3d(\n",
        "          x=x, y=y, z=z, mode='markers'))\n",
        "\n",
        "  if plane:\n",
        "    xmin = np.amin(x)\n",
        "    xmax = np.amax(x)\n",
        "    ymin = np.amin(y)\n",
        "    ymax = np.amax(y)\n",
        "\n",
        "    x1 = np.linspace(xmin, xmax, 100)\n",
        "    x2 = np.linspace(ymin, ymax, 100)\n",
        "\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    Y = predict_house_price(X1, X2, a)\n",
        "\n",
        "    fig.add_trace(go.Surface(\n",
        "        x=x1, y=x2, z=Y))\n",
        "\n",
        "  fig.update_layout(scene = dict(\n",
        "      xaxis_title=\"Location\",\n",
        "      yaxis_title=\"Size (sqm)\",\n",
        "      zaxis_title=\"Price (k)\"),\n",
        "      width=700,\n",
        "      margin=dict(r=20, b=10, l=10, t=10)\n",
        "  )\n",
        "  fig.show()\n",
        "\n",
        "def plot_3d_data_samples_with_fitted_plane(data, a):\n",
        "  plot_3d_data_samples(data, a, plane=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq70_ID_jCBq"
      },
      "source": [
        "# Simple linear model\n",
        "\n",
        "In a linear model, we assume that the *labels* we want to predict can be obtained through a *linear transformation* of the *features*.\n",
        "\n",
        "When we have only one feature, the linear model reduces to the equation of a line, which we are all familiar with:\n",
        "\n",
        "$$y = ax + b$$\n",
        "\n",
        "Here the feature is denoted by $x$, the label by $y$, and $(a, b)$ are the *parameters* of the model.\n",
        "\n",
        "The goal of the *linear regression* method, then, is to obtain the coefficients $(a, b)$ that best model the data (our features and labels)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_l1SNmKjJkB"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We will be using a subset of the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) with just 15 points, predicting the petal width from petal length of iris flowers.\n",
        "\n",
        "Here is some code to load the datasetâ€”we extract the petal widths into an array called `petal_width` and petal lengths into `petal_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WXxdH17IZCDY"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import Bunch\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "dataset = load_iris()\n",
        "# A list of examples we want to select from the iris dataset for demonstration purposes.\n",
        "sample = [ 57, 122, 118,  53, 117,  81,  70, 142,  84,  16, 103,  82,  66, 31,  83]\n",
        "\n",
        "# np.take selects specific examples from the array representing the full dataset.\n",
        "petal_length = np.take(dataset.data[:, 2], sample)\n",
        "petal_width = np.take(dataset.data[:, 3], sample)\n",
        "iris_data = Bunch(data=petal_length, target=petal_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "kBoHhSSFZeC2",
        "outputId": "aa352178-aaf8-45a2-ed58-b96a782893eb"
      },
      "outputs": [],
      "source": [
        "plot_iris_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HdgNHBHwPuv"
      },
      "source": [
        "## Fitting a simple linear model\n",
        "\n",
        "Our model is of the form\n",
        "\n",
        "$$y = ax + b$$\n",
        "\n",
        "or, in our case of the iris dataset,\n",
        "\n",
        "$$petal\\_width = a \\cdot petal\\_length + b$$\n",
        "\n",
        "for some unknown $a$ and $b$. Our goal will be to find the line (parameterised by $a$ and $b$) which fits our dataset the best.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwwQRvMmynC"
      },
      "source": [
        "## Experimenting with random line\n",
        "Try out some lines using another magic plotting function, setting different values for `a` and `b`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "S7K74g0UxpSd",
        "outputId": "29f33fe1-14b9-4b55-ceff-bacfd2940cc6"
      },
      "outputs": [],
      "source": [
        "a = 0.4\n",
        "b = 1\n",
        "plot_iris_dataset_with_line(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwWM8J_Y1ZCa"
      },
      "source": [
        "## Quantifying the residuals\n",
        "\n",
        "How could you tell whether one line you tried was better than another? You probably used some notion of \"closeness\" of the line to the data points. For example, the line\n",
        "\n",
        "![](https://raw.githubusercontent.com/kamilest/oxwocs-girls-who-ml-2021/main/images/bad_line.jpeg)\n",
        "\n",
        "probably looks worse than this other line below.\n",
        "\n",
        "![](https://raw.githubusercontent.com/kamilest/oxwocs-girls-who-ml-2021/main/images/better_line.jpeg)\n",
        "\n",
        "Can we quantify this error? The answer is yes, and we can do this by computing the *residuals*.\n",
        "\n",
        "The *residual* is just the *difference* between the value predicted by the model, $\\hat{y}_i$, and the ground truth value $y_i$ for some observation $i$. For the data points in the iris dataset, we can visualise the residuals (the differences between the petal widths predicted by the line and the petal widths in the dataset) as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "e7sfAc_O5YpC",
        "outputId": "05e25fce-80a5-41f8-bcdb-b51d7447bac6"
      },
      "outputs": [],
      "source": [
        "plot_iris_dataset_with_line(a=0.1, b=0, residuals=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKWRUHuj5la_"
      },
      "source": [
        "where the residuals are shown with the red lines. You can see that the total length of the red lines above is larger than that in the following plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "2v1bSweA1Xqa",
        "outputId": "e9216ad1-bbdf-4342-ac53-e7a8c1a8ade6"
      },
      "outputs": [],
      "source": [
        "plot_iris_dataset_with_line(a=0.4, b=-0.2, residuals=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoZ3z7XA563X"
      },
      "source": [
        "However, since we do not really care about whether the residual is above or below the line, and because we want  the penalty to scale faster for very large distances, we square the value of the residual:\n",
        "\n",
        "$$ (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "To account for all the information in the dataset and not just the single point, we compute the average of the squared residuals for all points (assuming there are $N$ points in total, and again accounting for the fact that Python is 0-indexed):\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=0}^{N-1} (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "which is, in fact, one of the most popular *loss functions*, or *performance measures*, in machine learning called the *mean squared error* (MSE). Our next task will be to compute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOOps6TxDbwR"
      },
      "source": [
        "## Task 1: Compute the MSE\n",
        "\n",
        "Recall that $\\hat{y}_i$ is the value predicted by the model $\\hat{y}_i = a x_i + b$, where $x_i$ in our case is the *petal length*, $y_i$ is the *petal width*.\n",
        "\n",
        "The final expression for the mean squared error is therefore\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=0}^{N-1} (a x_i + b - y_i)^2$$\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ Write a function to compute the mean squared error of a given line for the iris dataset.\n",
        "\n",
        "*Hint:* With all $x_i$ stored in `x` and all $y_i$ stored in `y`, the $x_i$ and $y_i$ in the dataset can be accessed through `x[i]` and `y[i]` respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8e5j7bQi8rKW"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(a, b, data):\n",
        "  x, y = data\n",
        "  # mse = None # TODO: write some code here\n",
        "  ###\n",
        "  ###\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTCT137utHXo"
      },
      "outputs": [],
      "source": [
        "\n",
        "mean_squared_error(a, b, iris_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4HUaaaDrIk"
      },
      "source": [
        "# Computing the analytic solution\n",
        "\n",
        "The best coefficients $a$ and $b$ that minimise the mean squared error can be found analytically, using a bit of calculus.\n",
        "\n",
        "In particular, we set the *gradient* of the MSE loss function to 0 in order to obtain the *least squares estimate* for $a$ and $b$. For MSE denoted by $L(a, b)$, setting\n",
        "\n",
        "$$ \\dfrac{\\partial L }{\\partial a} := 0 $$\n",
        "\n",
        "and\n",
        "\n",
        "$$ \\dfrac{\\partial L }{\\partial b} := 0 $$\n",
        "\n",
        "we obtain the least squares estimate\n",
        "\n",
        "$$ a = \\dfrac{\\sum_i(x_i - \\bar x)(y_i - \\bar y)}{\\sum_i(x_i - \\bar x)^2}$$\n",
        "&nbsp;\n",
        "$$ b = \\bar y - a \\bar x$$\n",
        "\n",
        "where $\\bar x$ is the mean value of $[x_0, \\dots, x_{N-1}]$ and so on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03vAlPwlBYWy"
      },
      "source": [
        "## Task 2: Compute the least squares estimate\n",
        "\n",
        "Use the above expressions for $a$ and $b$ that minimise the MSE to get the least squares coefficients for the iris dataset.\n",
        "* *Hint*: you might find it helpful to first compute the sum terms separately, e.g. compute `sum_xy`, `mean_x`, `mean_y` and use these variables in the expressions for `a` and `b`.\n",
        "\n",
        "Plot the resulting line using the magic function `plot_iris_dataset_with_line(a, b)` (and the residuals if you like).\n",
        "* How does this line fit the data?\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$ Compute the MSE for the coefficients you obtained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTIygp22GaQS"
      },
      "outputs": [],
      "source": [
        "def iris_least_squares_estimate(data):\n",
        "  x, y = data\n",
        "  # TODO: compute the values for the expressions for a and b in the previous\n",
        "  # section.\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  return a, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG7W9hNkHy6_"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the resulting line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXi3p3RfH_zP"
      },
      "outputs": [],
      "source": [
        "# TODO: Computing the MSE of the resulting line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVqXRuIC-nOU"
      },
      "source": [
        "## Task 3: Predict petal widths for new flowers\n",
        "\n",
        "In the previous sections, we found the coefficients `a` and `b` that best describe the dataset of our 15 observations. However, the main reason we do this estimation is so that we can predict the labels for *new*, unseen observations.\n",
        "\n",
        "To estimate how well our model *generalises* to unseen data, we normally use a *test* dataset. Let's first plot some test values in red, with the original values (our *training dataset*) in lighter shade:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "a_3C_TIh_sfI",
        "outputId": "252c53a6-bdc7-4840-b86a-d37373eb7182"
      },
      "outputs": [],
      "source": [
        "# Get indices for five test points from the full dataset.\n",
        "test_sample = [89, 20, 72, 67, 12]\n",
        "\n",
        "test_petal_length = np.take(dataset.data[:, 2], test_sample)\n",
        "test_petal_width = np.take(dataset.data[:, 3], test_sample)\n",
        "\n",
        "plot_iris_dataset(test=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUoWGykWCRxR"
      },
      "source": [
        "### Plot the estimated line\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$ Use one of the magic plotting functions to plot the regression line (which you found the parameters for using the least squares estimate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1nPEX8Dtzs"
      },
      "outputs": [],
      "source": [
        "# TODO: call a function to plot the regression line for the test dataset.\n",
        "###\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-E1y6YZCCuw"
      },
      "source": [
        "You may notice that the line does not exactly fit the new data points. We can print the labels predicted by the model and compare them to the ground truth labels, as well as compute the MSE for the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmN616pzGI47"
      },
      "source": [
        "### Predict the labels for test data\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$ Predict the labels for the test data points. Most of the code has been filled out for youâ€”you only need to compute the predictions using the `a` and `b` you estimated earlier.\n",
        "\n",
        "*Hint:* Recall that `y_predicted`, or $\\hat{y}_i$, is computed as\n",
        "\n",
        "$$ \\hat{y}_i = ax_i + b$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XxQLpkiGW5d"
      },
      "outputs": [],
      "source": [
        "def predict_petal_widths(a, b):\n",
        "  for (x, y_true) in zip(test_petal_length, test_petal_width):\n",
        "    ### # TODO: compute the prediction\n",
        "    ### Assign the prediction to variable y_predicted\n",
        "\n",
        "    print('Ground truth: {}\\tPrediction: {}'.format(y_true, y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiweVskIVv5"
      },
      "source": [
        "### Compute test MSE\n",
        "\n",
        "We can also compute the test mean squared error. Since the MSE function was conveniently dependent on the `data` argument, we only need to pass the test points instead of the training points to get the test MSE.\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$\n",
        "\n",
        "*Is the test MSE larger than the MSE you computed for the training data?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBP21jttIVKp"
      },
      "outputs": [],
      "source": [
        "data = (test_petal_length, test_petal_width)\n",
        "### # TODO: pass the relevant arguments here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obKObrELxQJZ"
      },
      "source": [
        "# Higher dimensional input features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCTTRUEX66-"
      },
      "source": [
        "The task is to fit a linear regression model to a dataset with\n",
        "higher dimensional input features. We choose to model the housing price as a fucntion of the property's location and its size, using a synthetic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Z3oKHZSmXoux",
        "outputId": "e9a5445b-d870-4077-c29f-bd0d975643a3"
      },
      "outputs": [],
      "source": [
        "# generate the housing price dataset\n",
        "location = np.array([1, 1, 2, 2, 3, 3, 4, 4, 4, 5])\n",
        "size = np.array([50, 90, 30, 40, 36, 45, 40, 78, 108, 200])\n",
        "price = np.array([300, 500, 300, 350, 450, 500, 400, 600, 800, 400])\n",
        "\n",
        "data_3d = (location, size, price)\n",
        "\n",
        "plot_3d_data_samples(data_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAMpRewExtHR"
      },
      "source": [
        "## Task 4: Fit Linear Regression to higher dimensional feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LruYrN13uF8z"
      },
      "source": [
        "### Create the big data matrix X\n",
        "\n",
        "Now *can you create the data matrix that stores all the input features?*\n",
        "\n",
        "Recall that\n",
        "\n",
        "$$\\mathbf X = \\begin{bmatrix} \\mathbf{x}_1^\\top \\\\ \\mathbf{x}_2^\\top \\\\ \\vdots \\\\ \\mathbf{x}_{N}^\\top \\end{bmatrix}, \\;\\;\\text{where}\\;\\; \\mathbf{x}_i=\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d\\end{bmatrix}$$\n",
        "\n",
        "Here we index the observations from $1$ to $N$ again, but remember that in code they would range from `0` to `N-1`.\n",
        "\n",
        "*Hint:* for vector and matrix operations, you can use a popular Python scientific computing library called NumPy:\n",
        "1. First, reshape each of the feature arrays (in this case `location` and `size`) into *column* vectors of shape `(N, 1)`.* For the NumPy array `x`, this can be done using `x.reshape()` function, passing your desired shape as the argument. You might also notice that `location` and `size` are NumPy arrays already, so no need to explicitly transform them into the NumPy format, just reshaping.\n",
        "2. Recall that we also needed to append ones to every feature vector (i.e. $\\mathbf x_i = [x_1, \\dots, x_d, 1]^\\top$) to avoid explicitly modelling the bias $b$. Alternatively, we can create another feature consisting of just ones using the `np.ones(shape)` function, where `shape` should be the same as the shape of your other column feature vectors.\n",
        "3. Concatenate all the reshaped feature vectors (`location`, `size`, and the vector of `ones` that you created) into a matrix $\\mathbf X$ using `np.concatenate((reshaped_location, reshaped_size, ones), axis=1)` function.\n",
        "4. Remember that `y` is the *column* vector of labels: $$ \\mathbf y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{N}\\end{bmatrix}$$ In our case, the labels are stored in the numpy array `price`. Don't forget, as before, to reshape it into a column!\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V2shMMCamBL"
      },
      "outputs": [],
      "source": [
        "N = len(location) # get the number of observations\n",
        "\n",
        "# TODO: create the X matrix\n",
        "###\n",
        "\n",
        "# TODO: create the y vector\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq2vfcvkbqVr"
      },
      "source": [
        "### Aside on row and column vectors\n",
        "\n",
        "Take a look at the entire matrix $\\mathbf X$:\n",
        "\n",
        "$$ \\mathbf X = \\begin{bmatrix}\n",
        "x_{1}^{(1)} & x_{2}^{(1)} & \\dots & x_{d}^{(1)} & 1 \\\\\n",
        "x_{1}^{(2)} & x_{2}^{(2)} & \\dots & x_{d}^{(2)} & 1 \\\\\n",
        "\\vdots & \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n",
        "x_{1}^{(N)} & x_{2}^{(N)} & \\dots & x_{d}^{(N)} & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this case $x_i^{(m)}$ means the $i$-th feature of the $m$-th observation in the dataset. Again note that I index the examples starting from 1 because it is easier to read, but in Python code they will all range from `0` to `N-1` for the observations, and from `0` to `d-1` for the features.\n",
        "\n",
        "Taking a look at the first row of the matrix,\n",
        "$$[x_{1}^{(1)}, x_{2}^{(1)}, \\dots, x_{d}^{(1)}, 1]$$\n",
        "\n",
        "you can see that it contains all the $d$ features (as well as the constant term $1$) for the first observation (superscript $\\phantom\\cdot^{(1)}$). But because the vectors are normally represented as columns (and the $i$-th observation $\\mathbf x^{(i)}$ is interpreted as a column vector), to put the column vector $\\mathbf x^{(i)}$ into the matrix $\\mathbf X$ as we just did, we need to transpose it into a row $\\mathbf x^{(i)\\top}$.\n",
        "\n",
        "Similarly, by taking a look at the first column\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "x_1^{(1)} \\\\\n",
        "x_1^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "x_1^{(N)} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "you can see that it contains the values for the first feature (subscript $\\cdot_1$) for all examples in the dataset (superscripts $\\phantom\\cdot^{(1)}$, ..., $\\phantom\\cdot^{(N)}$).\n",
        "\n",
        "Since our data is grouped by features (arrays `location` and `size`) and not by observations, we construct the big matrix `X` by combining the *columns* representing the different features, rather than the rows representing the different observations. By taking a look at the last column of $\\mathbf X$ you may also see why we needed to create a column vector of `ones`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYotidnv9_W"
      },
      "source": [
        "### Compute the least squares estimate **a**\n",
        "\n",
        "Now *can you compute the least squares estimate $\\mathbf{a}$?*\n",
        "\n",
        "Recall $$ \\mathbf{a} = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y $$\n",
        "\n",
        "*Hints*: For vector operations, use the following functions from the NumPy library.\n",
        "\n",
        "**Option 1**: use\n",
        "* `np.linalg.inv` for matrix inversion\n",
        "* `np.matmul` for matrix multiplication\n",
        "* `np.transpose` for matrix transpose\n",
        "\n",
        "and compute $\\mathbf a$ directly following the expression above.\n",
        "\n",
        "**Option 2**: observe that we can rewrite the above expression as\n",
        "\n",
        "$$ (\\mathbf X^\\top \\mathbf X) \\mathbf a = \\mathbf X^\\top \\mathbf y. $$\n",
        "\n",
        "Overloading the notation a bit, we can solve a generic system of equations `Ax = b` (where `x` is the unknown) using the function `np.linalg.solve(A, b)`. What are `A`, `x` and `b` in the expression $ (\\mathbf X^\\top \\mathbf X) \\mathbf a = \\mathbf X^\\top \\mathbf y $?\n",
        "\n",
        "As always, ask your demonstrator if unsure!\n",
        "\n",
        "$\\color{red}{\\textbf{TO DO :}}$ Compute Least Squares estimate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtC4KYsBmWvN"
      },
      "outputs": [],
      "source": [
        "# TODO: calculate the coefficient vector `a` using your preferred method.\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GIYxt75wWNW"
      },
      "source": [
        "Now plot the plane using the estimated coefficient $\\mathbf{a}$.\n",
        "\n",
        "*Is this plane a good fit to our data?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "oKTqFZfUpJq8",
        "outputId": "e7bf50d1-5ddf-46bf-948e-1847e30c642c"
      },
      "outputs": [],
      "source": [
        "plot_3d_data_samples_with_fitted_plane(data_3d, a)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
