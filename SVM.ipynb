{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T90cX4ichklP"
   },
   "source": [
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "One thing SVMs are very good at is text classification.\n",
    "\n",
    "The goal here is to determine whether a tweet was written by a Democratic or Republican politician, using just the text of the tweet.\n",
    "\n",
    "`sklearn` library is used in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICj3ClLyhklU"
   },
   "source": [
    "The data has three fields:\n",
    "\n",
    "| Feature \t| Description               \t|\n",
    "|---------\t|---------------------------\t|\n",
    "|   Party \t| Democrat or Republican    \t|\n",
    "|  Handle \t| The author's Twitter name \t|\n",
    "|   Tweet \t| The text of the tweet     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L6UDOfFbhklV",
    "outputId": "d578372b-fed8-4004-ec5e-72ee78b079c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/tweets.csv')\n",
    "\n",
    "# Training an SVM on a lot of data with a lot of features can take a few minutes,\n",
    "# so to keep things speedy here we will use a subset of the data.\n",
    "data = data.sample(5000, random_state=5)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e75pKcI0hklW"
   },
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agrR_Q1ThklW"
   },
   "source": [
    "How many politicians do we have tweets from, per party?\n",
    "\n",
    "(Not how many tweets per party!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PFYvrmmhklW",
    "outputId": "b42c8be8-714b-4435-d63b-6b6fa56f182b"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset='Handle')['Party'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn55DQqQhklX"
   },
   "source": [
    "And how many tweets per politician?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdOWdL_RhklX",
    "outputId": "f44a620d-3aec-42c5-9da1-9b7cc943e63e"
   },
   "outputs": [],
   "source": [
    "data.groupby('Handle')['Tweet'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piOmrVcShklY"
   },
   "source": [
    "## Working with text data\n",
    "\n",
    "The features for an SVM can't be words or whole tweets. We need a numerical representation for the words in the texts. One method is to transform the text into TF-IDF vectors.\n",
    "\n",
    "It will take the tweets, tokenise them into words (using a special tokeniser that knows how best to split up tweets), remove stop words (very common words like \"the\" and \"and\", which do not really contribute to the meaning of a tweet much) then it will create a sparse matrix representation of all the tweets. Each row is a single tweet, each column is a word in the vocabulary of all the tweets.\n",
    "\n",
    "It only uses the 5000 most common words - using all ~200k words would take a long time to train a model.\n",
    "\n",
    "(This will take a few seconds!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glRRmyW4hklY",
    "outputId": "c4f6c40d-aa1f-401d-9c7c-99eb46899408"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tok(text):\n",
    "    tt = TweetTokenizer()\n",
    "    return tt.tokenize(text)\n",
    "\n",
    "transformer = TfidfVectorizer(tokenizer=tok, stop_words='english', max_features=5000)\n",
    "tweet_vecs = transformer.fit_transform(data['Tweet'])\n",
    "\n",
    "tweet_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvC_PfIOhklZ",
    "outputId": "5374e6c7-c02b-4f30-87a4-61344c3f0322"
   },
   "outputs": [],
   "source": [
    "# Some words in the vocabulary and their IDs\n",
    "\n",
    "list(transformer.vocabulary_.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ynW7uv7hklZ"
   },
   "source": [
    "## Setting up the data\n",
    "\n",
    "As is standard, we will use some of our data for training the model and some of it for evaluating it. This gives a better idea of how well the model can generalise to unseen data, rather than simply overfitting to the data it has seen.\n",
    "\n",
    "First, we set up a variable `y` to store the Party labels we want to predict.\n",
    "\n",
    "Then, we use the `train_test_split` function to split up the `tweet_vecs` and the `y` data into train/test portions, using an 80:20 train:test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Gf3xtiYjhkla"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['Party']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweet_vecs, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i-l5eQ9hklb"
   },
   "source": [
    "## Task 1: Train a linear kernel SVM\n",
    "\n",
    "SVMs in `sklearn` have a few configurable options. The key ones are the kernel to be used (which can overcome non-separable data classes) and the regularization value $C$ (to relax or tighten the margins).\n",
    "\n",
    "Use a `for` loop to try different values for the kernel: `['linear', 'rbf', 'poly', 'sigmoid']`\n",
    "\n",
    "On each iteration, create a classifier with that `kernel`, and call `.fit()` with the training data.\n",
    "\n",
    "Then, use the `.score()` method to see how well the model did on both the seen and the unseen data. What do you observe?\n",
    "\n",
    "$\\color{red}{\\textbf{TO DO :}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSDFtighhklb",
    "outputId": "b3e2041b-ee08-4040-b49c-6db49cd487de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Your code here...\n",
    "\n",
    "###\n",
    "###\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2dVcl9Fhklb"
   },
   "source": [
    "## Task 2: Find the best model parameters\n",
    "\n",
    "In addition to testing different kernels, try different values for $C$, the regularization hyperparameter.\n",
    "\n",
    "Rather than doing this in a loop, one model at a time, we can parallelise it using `GridSearchCV` in `sklearn`.\n",
    "\n",
    "The `GridSearchCV` class takes a model, with a dictionary of hyperparameters and values. Then you just fit/train it as usual, using the training data from before.\n",
    "\n",
    "Try the following:\n",
    "\n",
    "1. Different kernels\n",
    "2. A few values for $C$\n",
    "\n",
    "Below, create a `GridSearchCV` in the same way you would do with a model: assign it to a variable named `gcv`, pass it the `classifier` as your basic model without parameters set, and also pass it `params`.\n",
    "\n",
    "To speed things up, set `n_jobs=-1` to use all available CPU cores. Set `verbose=1` so you get updates as it proceeds - useful for making sure it is actually working!\n",
    "\n",
    "$\\color{red}{\\textbf{TO DO :}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh2JFsl2hklc",
    "outputId": "3c96c67c-aa16-4e7a-d8ea-9ee49a1c2a82"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = dict(kernel=['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "              C=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7, 2.0],\n",
    "             )\n",
    "\n",
    "classifier = SVC()\n",
    "\n",
    "# Your code here...\n",
    "\n",
    "###\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AUn_Y9Shklc"
   },
   "source": [
    "### What was the best model?\n",
    "\n",
    "`GridSearchCV` evaluated each possible model using the accuracy metric.\n",
    "\n",
    "The best model is stored inside `gcv` as `best_estimator_`. Its score is in `gcv.best_score_` and the actual hyperparameters used are in `gcv.best_params_`.\n",
    "\n",
    "(The score here is not the score on the training set, but the average score across subsets of the training set.)\n",
    "\n",
    "Take a look at these and then evaluate the best model using the test set.\n",
    "\n",
    "How does it compare to the four models you trained before?\n",
    "\n",
    "$\\color{red}{\\textbf{TO DO :}}$ Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VnoKOCJfhklc"
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "###\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXg8JK5Thklc"
   },
   "source": [
    "### What is easier to classify? Democrat or Republican?\n",
    "\n",
    "Accuracy only gives one impression. We have three classes here, so print a classification report for each of the baseline models.\n",
    "\n",
    "`sklearn.metrics.classification_report` takes two arguments: the true labels and a model's predictions.\n",
    "\n",
    "You can get predictions for `X_test` by using the `.predict()` method of a trained model.\n",
    "\n",
    "How does the best model do at predicting the two classes?\n",
    "\n",
    "$\\color{red}{\\textbf{TO DO :}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5gMutKv5hklc"
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "###\n",
    "###\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
