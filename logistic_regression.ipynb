{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a79v7_LoGjUi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE9JO4UIQsrB"
      },
      "source": [
        "# Data generation\n",
        "\n",
        "We will first start by generating two clusters of data points. Then our task in this exercise is to define a boundary that separates the clusters, given a few observations from each cluster. When new data arrives, we can easily decide which cluster the new observation actually belongs to, i.e. **classifying** the new observation to one of the two clusters.   \n",
        "\n",
        "*Run the below 3 code blocks and observe the two clusters of data points in the plot.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2xm0nLclFuIJ"
      },
      "outputs": [],
      "source": [
        "cluster_centres = np.array([[-1, -3], [2, 2]])\n",
        "colour_list = ['red', 'blue']\n",
        "\n",
        "def generateData(num_samples_pc, dimensions=2, cluster_locs=cluster_centres):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for c, locs in enumerate(cluster_locs):\n",
        "        pos = np.random.randn(num_samples_pc, dimensions) + locs\n",
        "        data.append(pos)\n",
        "        labels.append(np.ones(num_samples_pc) * c)\n",
        "\n",
        "    data_np = np.concatenate(data, axis=0)\n",
        "    labels_np = np.concatenate(labels, axis=0)\n",
        "    return data_np, labels_np\n",
        "\n",
        "train_data, train_labs = generateData(100)\n",
        "test_data, test_labs = generateData(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "14U53mhaHSJJ"
      },
      "outputs": [],
      "source": [
        "def plot_data(data, labs, a=None, b=None, c=None, color=colour_list):\n",
        "  plt.xlim([-8, 8])\n",
        "  plt.ylim([-8, 8])\n",
        "  for ic in range(2):\n",
        "      ind_class = np.where(labs == ic)\n",
        "      #color = plt.cm.Set1(ic)\n",
        "      plt.scatter(data[ind_class, 0], data[ind_class, 1], s=10, color=color[ic], label=str(ic))\n",
        "\n",
        "  if a is not None and b is not None and c is not None:\n",
        "    x = np.linspace(-8, 8, 10)\n",
        "    y = ( - b * x - c) / a\n",
        "    plt.plot(x, y)\n",
        "\n",
        "  plt.ylabel(\"x2\")\n",
        "  plt.xlabel(\"x1\")\n",
        "  plt.gca().set_aspect('equal')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "pxTFdWoLJe3U",
        "outputId": "b7e9933c-62bb-41d4-8ccc-7665e366ef11"
      },
      "outputs": [],
      "source": [
        "plot_data(train_data, train_labs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxaJ7VIIKUdu"
      },
      "source": [
        "# Decision boundary\n",
        "\n",
        "* The decision boundary is a line which separates out the data points from different clusters.\n",
        "\n",
        "* If we have a data point $\\mathbf{x}$ that we want to classify, where $\\mathbf{x} = \\{x_1, x_2\\}$ -- a 2D feature vector. Then the predicted class depends on which side of the line $\\mathbf{x}$ lies.\n",
        "\n",
        "* We represent the decision boundary of the form: $f(\\mathbf{x}) = ax_1 + bx_2 + c = 0$.\n",
        "\n",
        "* This allows us to easily compute which side of the line the point lies from looking at the sign of $f(\\mathbf{x}) = ax_1 + bx_2 + c$.\n",
        "\n",
        "* A positive sign indicates that the data point $\\mathbf{x}$ belongs to the blue class, and a negative sign indicates red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Fj6A9LOhiW"
      },
      "source": [
        "## Sigmoid function\n",
        "\n",
        "* The sigmoid function is defined as: $\\sigma (t) =\\frac{1}{1 + exp(-t)}$\n",
        "\n",
        "* The sigmoid function has some very nice properties (see the lecture note). An important one is that the value of the sigmoid function always lies in between 0 and 1. Hence, it's a perfect tool to model probability distributions.\n",
        "\n",
        "* Here, we use the sigmoid function to represent the probability that a data point $\\mathbf{x}$ is from a certain class $C_i$, where $i$ is either 0 or 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAWi2hZ0RKPM"
      },
      "source": [
        "## Task 1: Implement sigmoid function\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ Implement the sigmoid function in the code block below.*\n",
        "\n",
        "Hint: Consider to use the `np.exp()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDrgBFXcPesU",
        "outputId": "0dec0d1f-584d-4f72-ec28-d601229abef0"
      },
      "outputs": [],
      "source": [
        "cluster_centres = np.array([[-1, -3], [2, 2]])\n",
        "\n",
        "def sigmoid(x):\n",
        "  ## TODO implement sigmoid in here ##\n",
        "  ####\n",
        "  ###\n",
        "  return sigmoid_value\n",
        "\n",
        "print(sigmoid(cluster_centres))\n",
        "\n",
        "print(cluster_centres + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kB2adaWQC1p"
      },
      "source": [
        "Let's plot the sigmoid function using the function below to see its shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "U9KDzYcORSVH",
        "outputId": "57b4dc46-5301-4264-8317-0aa9815d24da"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sigmoid_t = sigmoid(t)\n",
        "plt.plot(t, sigmoid_t, 'b-')\n",
        "plt.xlim([-10, 10])\n",
        "#plt.gca().set_aspect('equal')\n",
        "plt.xlabel(\"input t\")\n",
        "plt.ylabel(\"sigmoid value\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKe_G3tRP8D"
      },
      "source": [
        "Now can we combine the sigmoid function with the equation of our linear decision boundary to get a predictive probability for the class labels of a 2D data point?\n",
        "\n",
        "* Think about the output of the function $ay + bx + c$ and how that could work with the sigmoid function.\n",
        "\n",
        "* HINT: If a point lies on the line $ay + bx + c = 0$ what is the probability that it belongs to each class?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_kF_kaWRYk0"
      },
      "source": [
        "## Task 2: Predictive probability of a data point\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ Implement a function which gives the predictive probability $p(\\mathbf{x})$ of 2D data points $\\mathbf{x}=(x_1, x_2)$ being in the blue class for the given values for `a`, `b` and `c`. You will need to use the sigmoid function and evaluate $ay + bx + c$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6AbqNfASifE",
        "outputId": "1e3b7e73-3cb2-4e6e-f5fb-a405e3667830"
      },
      "outputs": [],
      "source": [
        "def predictive_prob(x, a, b, c):\n",
        "  ## TODO ##\n",
        "  ####\n",
        "  ####\n",
        "  ## TODO end ##\n",
        "  return pred_prob\n",
        "\n",
        "print(cluster_centres)\n",
        "print(cluster_centres[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BghJPZtnVByb"
      },
      "source": [
        "# Learning Objective: Binary Cross-Entropy Loss\n",
        "\n",
        "* We want to find a way to automatically infer the decision boundary.\n",
        "\n",
        "* The best decision boundary is the one which will assign a low probability of a data point being `blue` if it actually belongs to the `red` class and a probability of a data point being `red` if it does belong to the `red` class.\n",
        "\n",
        "* We use the Binary Cross Entropy loss to work out the decision boundary.\n",
        "\n",
        "$$\n",
        "L = \\sum_{i=1}^{N}- y_i\\log(p(\\mathbf{x}_i)) - (1 - y_i)\\log(1 - p(\\mathbf{x}_i))\n",
        "$$\n",
        "\n",
        "* Where $y_i$ represent $i$th element of `train_labs`, which is a vector of 0s and 1s. Here 0 indicates that the corresponding $i$th data point in `train_data` is `red` and 1 indicates that it is `blue`. $\\mathbf{x}_i$ is a data point in `train_data` and $p(\\mathbf{x}_i)$ represents our predictive probability that $\\mathbf{x}_i$ is blue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HC1cVNGRhrA"
      },
      "source": [
        "## Task 3: Implement the binary cross entropy loss.\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ Implement the BCE loss in the function below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4YSSymZW1Da"
      },
      "outputs": [],
      "source": [
        "def bce_loss(pred_probs, y):\n",
        "  ## TODO ##\n",
        "  ###\n",
        "  ###\n",
        "  ## TODO end ##\n",
        "  return bce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhhbVO6OdrtB"
      },
      "source": [
        "### Is BCE a good metric for how well our decision boundary seperates our data?\n",
        "\n",
        "Play around with the vlaues of `a`, `b`, and `c` below to see how they effect the values of BCE loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "KeiRbWOId3_U",
        "outputId": "affcc688-3ef7-4a2d-8eda-12ce1feb6069"
      },
      "outputs": [],
      "source": [
        "a=5\n",
        "b=5\n",
        "c=1\n",
        "\n",
        "plot_data(train_data, train_labs, a, b, c)\n",
        "print(\"BCE loss: \" + str(bce_loss(predictive_prob(train_data, a, b, c), train_labs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSaalIBMYidn"
      },
      "source": [
        "# Optimization with Gradient Descent (GD)\n",
        "\n",
        "* Gradient descent is a way to minimize (or maximize) a function, just think of it as walking down a hill.\n",
        "\n",
        "* If you want to walk down a hill from a random point, you'd choose a direction which points down, and then take a step.\n",
        "\n",
        "* That's what we do a lot in machine learning (literally, this is what everyone uses all the time), but rather than take a physical step, we just move the parameters by a certain amount which we call a step.\n",
        "\n",
        "* For GD to be useful, we need to calculate the gradient of our loss function.\n",
        "\n",
        "* The gradients for BCE are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial a} = \\sum_{i=1}^{N}(\\sigma(ax_1^i + bx_2^i + c) - y^i)x_1^i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial b} = \\sum_{i=1}^{N}(\\sigma(ax_1^i + bx_2^i + c) - y^i)x_2^i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial c} = \\sum_{i=1}^{N}(\\sigma(ax_1^i + bx_2^i + c) - y^i)\n",
        "$$\n",
        "\n",
        "Note that $(\\mathbf{x}^i, y^i)$ refer to the $i$-th data points in our observations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDx48onIRsqm"
      },
      "source": [
        "## Task 4: Implement the gradient of $a, b$ and $c$\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ Complete the below code block to implement a function which returns the gradients of $a, b$ and $c$.\n",
        "\n",
        "You can return multiple values from a function using `return grad_a, grad_b, grad_c`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaBapwlKawaE"
      },
      "outputs": [],
      "source": [
        "def compute_gradients(x, a, b, c, y):\n",
        "  ## TODO ##\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ## TODO end ##\n",
        "  return grad_a, grad_b, grad_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMbKj5zqcA3Q"
      },
      "source": [
        "## Task 5: Implement the gradient descent algorithm.\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ You will need the following equation to take a step:\n",
        "\n",
        "$$\n",
        "a^{t+1} = a^{t} - \\alpha\\cdot\\frac{\\partial{L}}{\\partial a^{t}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "pqomEdGwchuR",
        "outputId": "64de5a19-7e36-4334-9a2e-76cc632b5936"
      },
      "outputs": [],
      "source": [
        "def optimize(train_data, train_labs, num_steps):\n",
        "    a, b, c = 2, -1, -5\n",
        "    lr = 5e-3\n",
        "    for i in range(num_steps):\n",
        "      ## TODO ##\n",
        "      ###\n",
        "      ###\n",
        "      ###\n",
        "      ## TODO end ##\n",
        "\n",
        "      if i % 10 == 0:\n",
        "        print(\"Iteration %i\\tLoss %.3f\" % (i, bce_loss(predictive_prob(train_data, a, b, c), train_labs)))\n",
        "\n",
        "    plot_data(train_data, train_labs, a, b, c)\n",
        "\n",
        "    return a, b, c\n",
        "\n",
        "\n",
        "a_opt, b_opt, c_opt = optimize(train_data, train_labs, 100)\n",
        "print(\"The optimised parameters are: a=\" + str(a_opt) + \", b=\" + str(b_opt) + \", c=\" + str(c_opt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awqzNnyJgfvB"
      },
      "source": [
        "### What do you notice about the loss value? Is it doing what you expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFRk6isvgHtk"
      },
      "source": [
        "# Evaluating a Classifier\n",
        "\n",
        "We have now found out the parameters of a linear classifier. How well does it perform on the classification task on the 2D points? On what metric should we evaluate it on? Should we evaluate it on the **training set**, a set of data points that we use to estimate the classifier's parameters? Or should it be evaluated on a held-out **test set**, a set of data points that are reserved from training and only used for evaluating the model performance? What is the importance of having these two separate sets?\n",
        "\n",
        "We explore these questions in this section.\n",
        "\n",
        "\n",
        "\n",
        "*   We often use the accuracy of a classifier as the performance metric.\n",
        "*   It is defined as: $ \\text{Accuracy} = \\frac{\\text{Number of correct prediction}}{\\text{Number of total prediction}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYBJxgcsgKTT"
      },
      "source": [
        "## Task 6: Test set\n",
        "\n",
        "To evaluate a classifier, we first need to use it to make some predictions. At the beginning of this notebook, we generated some test data alongside of the training data. Check in the Data Generation section and look for two variables called `test_data` and `test_labs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8632grodvgH",
        "outputId": "e980771c-8a3d-44ff-9b08-00168006d337"
      },
      "outputs": [],
      "source": [
        "print(test_data.shape, test_labs.shape)\n",
        "print(\"Test data:\", test_data)\n",
        "print(\"Test labels:\", test_labs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmzATjZYgPQx"
      },
      "source": [
        "### Make prediction\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ can you make predictions using the $a, b, c$ values that you've found in Task 4b? Please complete the below code block.*  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxXvPWX1gTvr",
        "outputId": "c8b3737f-c113-4ad2-ed1b-c128d445c5fb"
      },
      "outputs": [],
      "source": [
        "def predict_labels(x, a, b, c):\n",
        "  ## TODO ##\n",
        "  ###\n",
        "  ###\n",
        "  ## TODO end ##\n",
        "  return pred_labels\n",
        "\n",
        "\n",
        "test_labs_predicted = predict_labels(test_data, a=a_opt, b=b_opt, c=c_opt)\n",
        "print(\"Predictions: \" , test_labs_predicted)\n",
        "print(\"Real labels: \", test_labs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8SYiclXgWUV"
      },
      "source": [
        "### Compute accuracy\n",
        "\n",
        "With the predicted labels `test_labs_predicted` from the previous task and the ground truth test labels `test_labs`, can you now evaluate the classifier's accuracy?\n",
        "\n",
        "* $\\color{red}{\\textbf{TO DO :}}$ complete the below code block that computes the classifier's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMJ_ngW-gVh8"
      },
      "outputs": [],
      "source": [
        "## TODO ##\n",
        "accuracy = ###\n",
        "## TODO end ##\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqbmHAb3pPpe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
